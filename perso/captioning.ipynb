{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Captioning images using CNN and RNN**\n\nThe problem is posed as follows: Given an image, we want to obtain a sentence that describes what the image consists of.\n\nThe solution I propose in this notebook consist of an Encoder and Decoder Model.\nThe Encoder is a ConvNet and the decoder is a LSTM RNN model which performed the image captioning.\nMy solution was inspired from the amazing medium article written by Stepan Ulyanin [medium article](https://medium.com/@stepanulyanin/captioning-images-with-pytorch-bc592e5fd1a3)\n\nThe dataset, I'm going to use is the COCO dataset for training the model. Every image comes with 5 different captions produced by different humans, as a result, every caption is slightly different from the other captions for the same image"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Pytorch libraries \nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms, models","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Graphic library\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport matplotlib.pyplot as plt","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Config matplotlib\nimport matplotlib as mpl\nmpl.rcParams['axes.grid'] = False\nmpl.rcParams['image.interpolation'] = 'nearest'\nmpl.rcParams['figure.figsize'] = 15, 25","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef show_dataset(dataset, n=6):\n  img = np.vstack((np.hstack((np.asarray(dataset[i][0]) for _ in range(n)))\n                   for i in range(5)))\n  plt.imshow(img)\n  plt.axis('off')","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Encoder**\n\nTo encode the images in inputs, we are going to use a ConvNet. The must better choice is using transfer learning to perform this kind of task. Hence, we are going to use the DenseNet121 model pretrained on the ImageNet dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"class EncoderConvNet(nn.Module):\n    def __init__(self, embed_size=1024):\n        super(EncoderConvNet, self).__init__()\n        # Get the densenet121 pretrained model\n        self.densenet = models.densenet121(pretrained=True)\n        # The densenet classifiers' layer is as follows :\n        # (classifier) : Linear(in_features=1024, out_features=1000, bias=True)\n        # Let's defined a fully connected layer\n        self.fc = nn.Linear(in_features=1024, out_features=embed_size)\n        # Let's defined a dropout layer\n        self.dropout = nn.Dropout(p=0.5)\n        # Let's defined the activation layer.\n        self.prelu = nn.PReLU()\n        #Applies the element-wise function: PReLU(x)=max(0,x)+aâˆ—min(0,x)\n    \n    def forward(self, images):\n        #\n        outputs = self.dropout(self.prelu(self.densenet(images)))\n        #\n        outputs = self.fc(outputs)\n        \n        return outputs","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Decoder**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n        super(DecoderRNN, self).__init__()\n        # define the properties\n        self.embed_size = embed_size\n        self.hidden_size = hidden_size\n        self.vocab_size = vocab_size\n        \n        # lstm cell\n        self.lstm_cell = nn.LSTMCell(input_size=embed_size, hidden_size=hidden_size)\n    \n        # output fully connected layer\n        self.fc_out = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size)\n    \n        # embedding layer\n        self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size)\n    \n    def forward(self, features, captions):\n        # batch size\n        batch_size = features.size(0)\n        \n        # init the hidden and cell states to zeros\n        hidden_state = torch.zeros((batch_size, self.hidden_size)).cuda()\n        cell_state = torch.zeros((batch_size, self.hidden_size)).cuda()\n    \n        # define the output tensor placeholder\n        outputs = torch.empty((batch_size, captions.size(1), self.vocab_size)).cuda()\n\n        # embed the captions\n        captions_embed = self.embed(captions)\n        \n        # pass the caption word by word\n        for t in range(captions.size(1)):\n\n            # for the first time step the input is the feature vector\n            if t == 0:\n                hidden_state, cell_state = self.lstm_cell(features, (hidden_state, cell_state))\n                \n            # for the 2nd+ time step, using teacher forcer\n            else:\n                hidden_state, cell_state = self.lstm_cell(captions_embed[:, t, :], (hidden_state, cell_state))\n            \n            # output of the attention mechanism\n            out = self.fc_out(hidden_state)\n            \n            # build the output tensor\n            outputs[:, t, :] = out\n    \n        return outputs","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# how many samples per batch to load\nbatch_size = 48\n# percentage of training set to use as validation\nvalid_size = 0.3\n\n#train_data = datasets.Flickr8k(root='data', ann_file='captions', transform=transforms.ToTensor())","execution_count":18,"outputs":[{"output_type":"error","ename":"IsADirectoryError","evalue":"[Errno 21] Is a directory: 'captions'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-1667772f9b4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalid_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlickr8k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mann_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'captions'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision/datasets/flickr.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, ann_file, transform, target_transform)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# Read annotations and store in a dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlickr8kParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mann_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'captions'"]}]},{"metadata":{},"cell_type":"markdown","source":"**Training**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}